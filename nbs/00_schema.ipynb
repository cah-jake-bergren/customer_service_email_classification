{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# schema\n",
    "\n",
    "> Our Pydantic Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "from typing import Dict, Callable\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "import time\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "\n",
    "import vertexai as vai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from vertexai.language_models._language_models import MultiCandidateTextGenerationResponse\n",
    "from google.cloud.aiplatform import BatchPredictionJob\n",
    "from google.cloud import storage\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "\n",
    "# GRPC requires this\n",
    "os.environ[\"GRPC_DNS_RESOLVER\"] = \"native\"\n",
    "\n",
    "PROJECT_ID = \"cdejam-gbsrc-ext-cah\"\n",
    "PROJECT_BUCKET = \"pharma_email_classification\"\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_embedder() -> VertexAIEmbeddings:\n",
    "    return VertexAIEmbeddings(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "        model_name='textembedding-gecko'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "WRITE_PREFIX = \"JDB_experiments\"\n",
    "\n",
    "DEFAULT_PREDICT_PARAMS = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "}\n",
    "\n",
    "_VERTEX_INITIATED = False\n",
    "ONE_MINUTE = 60\n",
    "\n",
    "\n",
    "def quota_handler(func: Callable):\n",
    "    @wraps(func)\n",
    "    def handle_quota(*args, **kwargs):\n",
    "        \"\"\"Handles GCP ResourceExhausted exceptions. \n",
    "        Will sleep the thread until the next minute before trying again.\"\"\"\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except ResourceExhausted:\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except ResourceExhausted:\n",
    "                    # Sleep until the next minute\n",
    "                    sleep_time = 60 - datetime.utcnow().second\n",
    "                    time.sleep(sleep_time)\n",
    "    return handle_quota\n",
    "\n",
    "\n",
    "def get_storage_client() -> storage.Client:\n",
    "    return storage.Client(project=PROJECT_ID)\n",
    "\n",
    "\n",
    "def init_vertexai(\n",
    "    project_id: str = PROJECT_ID,\n",
    "    region: str = REGION):\n",
    "    global _VERTEX_INITIATED\n",
    "    if not _VERTEX_INITIATED:\n",
    "        vai.init(project=project_id, location=region)\n",
    "        _VERTEX_INITIATED = True\n",
    "\n",
    "\n",
    "def get_model() -> TextGenerationModel:\n",
    "    init_vertexai()\n",
    "    return TextGenerationModel.from_pretrained(\"text-bison\")\n",
    "\n",
    "\n",
    "@sleep_and_retry\n",
    "@limits(calls=50, period=ONE_MINUTE)\n",
    "def predict(\n",
    "        prompt: str,\n",
    "        parameters: Dict[str, str] = DEFAULT_PREDICT_PARAMS\n",
    "        ) -> MultiCandidateTextGenerationResponse:\n",
    "    model = get_model()\n",
    "    return model.predict(\n",
    "        prompt,\n",
    "        **parameters)\n",
    "\n",
    "\n",
    "def batch_predict(\n",
    "        source_uri: str,\n",
    "        destination_uri_prefix: str,\n",
    "        model_parameters: Dict[str, str] = DEFAULT_PREDICT_PARAMS\n",
    ") -> BatchPredictionJob:\n",
    "    \"\"\"\n",
    "    Make a batch prediction request to text-bison.\n",
    "\n",
    "    :param source_uri: Source file in GCS with prompted requests, \n",
    "        I.E. 'gs://BUCKET_NAME/test_table.jsonl'\n",
    "    :param destination_uri_prefix: Where the results will be written, \n",
    "        ex: 'gs://BUCKET_NAME/tmp/2023-05-25-vertex-LLM-Batch-Prediction/result3'\n",
    "    \"\"\"\n",
    "    model = get_model()\n",
    "    batch_prediction_job = model.batch_predict(\n",
    "        source_uri=[source_uri],\n",
    "        destination_uri_prefix=destination_uri_prefix,\n",
    "        # Optional:\n",
    "        model_parameters=model_parameters\n",
    "    )\n",
    "    # print(batch_prediction_job.display_name)\n",
    "    # print(batch_prediction_job.resource_name)\n",
    "    # print(batch_prediction_job.state)\n",
    "    return batch_prediction_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
