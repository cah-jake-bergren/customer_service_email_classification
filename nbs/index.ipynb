{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from classifier import schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classifier\n",
    "\n",
    "> Classifying customer service emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will become your README and also the index of your documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "1. Clone the repo\n",
    "2. In the terminal, navigate to the project directory\n",
    "3. Create a virtualenv with at least python version 3.11\n",
    "4. Install via `pip install '.'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the existing model is likely failing\n",
    "\n",
    "My hypothesis is that the existing ML solution gained a lot of predictive power by utilizing specific keywords in the training corpus. Whether those keywords are predictive, or the usage of those keywords have lessened, the existing model is suffering because it essentially biased itself on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why prompting doesn't work well\n",
    "\n",
    "Prompting on a blob of email text struggles for a few likely reasons. One is that email text can be full of extraneous, inconsequential \"stuff\". Filtering text for what is essential to the conversation might help this.\n",
    "\n",
    "Another reason is that the LLM doesn't really understand these labels well. It wasn't trained to understand cardinal businesses. It is essentially a layman when it comes to CAH business process. It simply can't know what email text belongs to a category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My approach\n",
    "\n",
    "1. Load emails\n",
    "2. Prompt individual emails to remove boilerplate text (\"Summarize the conversation of this email as a series of steps\") using map-reduce (so we may handle larger than context examples).\n",
    "3. Build a chroma vector database of embedded training instances using the concatenated results of [2] and the existing labels.\n",
    "4. Query the chroma database for similar, labeled instances of data and pass them plus the summarized email to the LLM for a final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebooks\n",
    "\n",
    "- `00_schema.ipynb` - Pydantic objects\n",
    "- `01_load.ipynb` - Load our documents from GCS\n",
    "- `02_process.ipynb` - Process emails according to step 2 above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
